## 20.1 Решающие деревья. Функции информационного выигрыша. Алгоритм построения дерева.

**Решающие деревья** (или дерево решений) представляют собой алгоритм машинного обучения, основанный на правилах, что обеспечивает их **интерпретируемость**. Это означает, что мы можем понять как результаты построения дерева, так и процесс принятия решений. Одним из преимуществ является отсутствие необходимости в предобработке признаков, таких как нормировка или one-hot encoding, что позволяет работать как с числовыми, так и с категориальными данными. Решающие деревья также устойчивы к изменениям в данных[1][2].

### Функции информационного выигрыша

**Информационный выигрыш (IG)** используется для оценки полезности разбиения данных в узле дерева. Он определяется по формуле:

$$
IG = \frac{|X_{\text{node}}|}{|X_{\text{total}}|} I(X_{\text{node}}) - \frac{|X_{\text{right}}|}{|X_{\text{total}}|} I(X_{\text{right}}) - \frac{|X_{\text{left}}|}{|X_{\text{total}}|} I(X_{\text{left}})
$$

где $$I$$ — это функция нечистоты (impurity), которая показывает, насколько узел "чист" (т.е. содержит объекты одного класса). Чем ближе значение к одному классу, тем меньше нечистота.

1. **Misclassification Error**:

$$
I_E(X) = 1 - \max\{p(y)\} = 1 - \max_y \left( \frac{|\{x_i: y_i = y\}|}{|X|} \right)
$$

2. **Entropy**:

$$
I_H(X) = - \sum_{y \in Y} p(y) \log_2(p(y)) = - \sum_{y \in Y} \frac{|\{x_i : y_i = y\}|}{|X|} \times \log_2 \left( \frac{|\{x_i : y_i = y\}|}{|X|} \right)
$$

3. **Gini Impurity**:

$$
I_G(X) = \sum_{y \in Y} p(y)(1 - p(y)) = \sum_{y \in Y} \left( \frac{|\{x_i : y_i = y\}|}{|X|} \right) \left( \frac{|\{x_i : y_i \neq y\}|}{|X|} \right)
$$

<img src=https://github.com/BogruAKVD/ml-questions/blob/master/tickets/images/tickets20_1.png width="600" align="center">\
Для задач классификации обычно используется **Gini**, а для регрессии применяется другая метрика:

$$
I_V(X) = \sum_{x_i \in X} \sum_{x_j \in X} \frac{1}{2} (y_i - y_j)^2
$$

### Алгоритм построения дерева

1. Если все объекты в узле принадлежат одному классу, помечаем лист как этот класс и останавливаемся.
2. Ищем правило с наибольшим IG. Если ни одно правило не дает прироста информации, помечаем узел как принадлежащий к наибольшему классу и останавливаемся.
3. Разделяем узлы на детей по правилу.
4. Повторяем шаг 1 для каждого нового узла.

В случае регрессии возвращаем среднее значение вместо класса.

## 20.2 Глобальный поиск. Байесовская оптимизация. Функции выбора следующего измерения.

**Байесовская оптимизация** применяется в ситуациях, когда проведение множества экспериментов невозможно, например, при бурении нефтяных скважин. Основная идея заключается в проведении эксперимента и пересчете распределения значений функции на основе предыдущих экспериментов для оценки вероятности каждого значения. Хорошо подходит нормальное распределение для значений функции в точке. Чем ближе точка к измерению, тем ближе её матожидание к значению измерения и меньше дисперсия.

<img src=https://github.com/BogruAKVD/ml-questions/blob/master/tickets/images/tickets20_2.png width="600" align="center">

### Стратегии выбора точки

Функции выбора определяют новую точку для измерения:

1. **Ожидаемое улучшение (Expected Improvement)**:
 
$$
EI(x) = \mathbb{E} [\max(f(x) - f(x_{\text{best}}), 0)]
$$

3. **Верхняя граница доверительного интервала (Upper Confidence Bound)**:
 
$$
UCB(x) = \mu(x) + \beta\sigma(x)
$$

   где $\beta$ — параметр, определяемый пользователем.

3. **Вероятность улучшения (Probability of Improvement)**:

$$
PI(x) = P(f(x) > f(x_{\text{best}}))
$$


## Предсказание на доп вопросы
### 1
В регрессии количество листьев в дереве или $$2^{\text{глубина}}$$ определяет количество значений функции, так как каждый лист дает новое значение на каком-то промежутке.

Виды регуляризации деревьев (на самом деле другой билет):

1. Ограничение глубины дерева.
2. Минимальное количество точек в листе (разделяем новый лист, если в нем больше $$k$$ точек).
3. Минимальное количество точек в узле (начинаем разделять, если в ней больше $$k$$ точек).

Меньшее дерево снижает риск переобучения.

### 2
Как сделать первое измерение и какие начальные параметы (\mu(x) и \sigma(x)). Ответ: Я точный ответ не знаю, но сказал бы так. Сделаем первое в одном краю. Значение функции это начальное значение для матожидания. Чем ближе к краю тем меньше дисперсия вплоть до 0. Осталась проблема. Для некоторых функций выбора бесконечно много точек с лучшим значением, т.к. одиннаковое матожидание. По всем функциям выбора другой край одна из наилучших. Давайте для второго измерения возьмём её. Теперь у всех точек разные дисперсии и матожидания. Можем работать по алгоритму
